apiVersion: v1
kind: ConfigMap
metadata:
  name: logging-cfg
  namespace: default
data:
  elasticsearch.yml: |
    cluster.name: ${cluster_name}
    #network.host: 0.0.0.0
    http.host: 0.0.0.0
    transport.host: 127.0.0.1
    # minimum_master_nodes need to be explicitly set when bound on a public IP
    # set to 1 to allow single node clusters
    # Details: https://github.com/elastic/elasticsearch/pull/17288
    discovery.zen.minimum_master_nodes: 1

    bootstrap.memory_lock: ${bootstrap_memory_lock}
    xpack.security.enabled: false
    xpack.watcher.enabled: false
    xpack.graph.enabled: false
    xpack.monitoring.enabled: false
  logstash.yml: |
    ## Default Logstash configuration from logstash-docker.
    ## from https://github.com/elastic/logstash-docker/blob/master/build/logstash/config/logstash.yml
    #
    http.host: "0.0.0.0"
    path.config: /usr/share/logstash/pipeline

    ## Disable X-Pack
    ## see https://www.elastic.co/guide/en/x-pack/current/xpack-settings.html
    xpack.monitoring.enabled: false
  logstash.conf: |
    # all input will come from filebeat, no local logs
    input {
      beats {
        port => 5000
      }
    }

    ## some more advanced filtering and tagging of incoming kubernetes logs is done in logstash
    filter {
      if [type] == "kube-logs" {
        mutate {
          rename => ["log", "message"]
          add_tag => [ "docker" ]
              add_field => [ "cluster.name","nec" ]
        }

        date {
          match => ["time", "ISO8601"]
          remove_field => ["time"]
        }

        # all standard container logs files match a common pattern
        grok {
            match => { "source" => "/var/lib/docker/containers/%{DATA:container_id}/%{DATA:container_id}-json.log" }
            remove_field => ["source"]
        }

        # system services have a simpler log filename format that does not include namespace, pod names, or container ids
        grok {
            match => { "source" => "/var/log/hostlogs/%{DATA:container_name}.log" }
            add_field => { "namespace" => "default" }
            remove_field => ["source"]
        }
        
        json { source => "message" }
      }
    }

    output {
        elasticsearch {
            hosts => [ "elasticsearch-service:9200" ]
        }
    }
  kibana.yml: |
    ## Default Kibana configuration from kibana-docker.
    ## from https://github.com/elastic/kibana-docker/blob/master/build/kibana/config/kibana.yml
    #
    server.name: kibana
    server.host: "0"
    elasticsearch.url: http://elasticsearch-service:9200

    ## Disable X-Pack
    ## see https://www.elastic.co/guide/en/x-pack/current/xpack-settings.html
    ##     https://www.elastic.co/guide/en/x-pack/current/installing-xpack.html#xpack-enabling
    #
    xpack.security.enabled: false
    xpack.monitoring.enabled: false
    xpack.ml.enabled: false
    xpack.graph.enabled: false
    xpack.reporting.enabled: false
  log4j2.properties: |
    status = error
    appender.console.type = Console
    appender.console.name = console
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n
    rootLogger.level = info
    rootLogger.appenderRef.console.ref = console
  filebeat.yml: |
    filebeat.registry_file: /var/log/containers/filebeat_registry

    filebeat.prospectors:
    - input_type: log
      paths:
        - "/var/lib/docker/containers/*/*.log"
      symlinks: true
      json.keys_under_root: true
      json.add_error_key: true
      json.message_key: log
      multiline.pattern: '^\s'
      multiline.match: after
      document_type: kube-logs

    output.logstash:
      hosts: ["logstash-service:5000"]
      timeout: 15
    # Available log levels are: critical, error, warning, info, debug
    logging.level: info
